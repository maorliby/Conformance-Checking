{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a4c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.4)\n",
      "Requirement already satisfied: pulp in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.2.1)\n",
      "Requirement already satisfied: pm4py in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.7.16)\n",
      "Requirement already satisfied: requests in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: deprecation in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (2.1.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (3.4.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (0.21)\n",
      "Requirement already satisfied: wheel in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (0.45.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (77.0.3)\n",
      "Requirement already satisfied: intervaltree in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (3.1.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (6.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (3.10.1)\n",
      "Requirement already satisfied: pydotplus in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (1.15.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (4.67.1)\n",
      "Requirement already satisfied: cvxopt in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (1.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deprecation->pm4py) (24.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from intervaltree->pm4py) (2.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (3.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm->pm4py) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy pulp pm4py requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343ae0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cbcpy (from versions: none)\n",
      "ERROR: No matching distribution found for cbcpy\n"
     ]
    }
   ],
   "source": [
    "pip install cbcpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616bda22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pm4py[full] in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.7.16)\n",
      "Requirement already satisfied: numpy in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (2.2.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (2.2.3)\n",
      "Requirement already satisfied: deprecation in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (2.1.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (3.4.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (0.21)\n",
      "Requirement already satisfied: wheel in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (0.45.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (77.0.3)\n",
      "Requirement already satisfied: intervaltree in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (3.1.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (6.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (3.10.1)\n",
      "Requirement already satisfied: pydotplus in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (2.0.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (2025.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (1.15.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (4.67.1)\n",
      "Requirement already satisfied: cvxopt in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py[full]) (1.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deprecation->pm4py[full]) (24.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from intervaltree->pm4py[full]) (2.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py[full]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py[full]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py[full]) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py[full]) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py[full]) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py[full]) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py[full]) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.7->matplotlib->pm4py[full]) (1.17.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->pm4py[full]) (2025.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm->pm4py[full]) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pm4py 2.7.16 does not provide the extra 'full'\n"
     ]
    }
   ],
   "source": [
    "pip install pm4py[full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6750c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pm4py in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.7.16)\n",
      "Requirement already satisfied: numpy in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (2.2.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (2.2.3)\n",
      "Requirement already satisfied: deprecation in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (2.1.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (3.4.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (0.21)\n",
      "Requirement already satisfied: wheel in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (0.45.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (77.0.3)\n",
      "Requirement already satisfied: intervaltree in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (3.1.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (6.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (3.10.1)\n",
      "Requirement already satisfied: pydotplus in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (2.0.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (2025.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (1.15.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (4.67.1)\n",
      "Requirement already satisfied: cvxopt in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pm4py) (1.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from deprecation->pm4py) (24.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from intervaltree->pm4py) (2.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib->pm4py) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.7->matplotlib->pm4py) (1.17.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->pm4py) (2025.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm->pm4py) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae7e3b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (77.0.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a0c681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Classes.py to src/\n",
      "Downloaded icpm_experiments.py to src/\n",
      "Downloaded preprocessing.py to src/\n",
      "Downloaded RunningHorizon.py to src/\n",
      "Downloaded utils.py to src/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "if not os.path.exists(\"src\"):\n",
    "    os.makedirs(\"src\")\n",
    "\n",
    "src_files = [\"Classes.py\", \"icpm_experiments.py\", \"preprocessing.py\", \"RunningHorizon.py\", \"utils.py\"]\n",
    "base_url = \"https://raw.githubusercontent.com/maorliby/Conformance-Checking/main/src/\"\n",
    "\n",
    "for file_name in src_files:\n",
    "    response = requests.get(f\"{base_url}{file_name}\")\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"src/{file_name}\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {file_name} to src/\")\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e07a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulp already installed.\n",
      "pm4py already installed.\n",
      "numpy already installed.\n",
      "pandas already installed.\n",
      "requests already installed.\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pulp\n",
    "import time\n",
    "import requests\n",
    "from io import StringIO\n",
    "from pm4py.objects.petri_net.utils import petri_utils\n",
    "from pm4py.objects.petri_net.obj import PetriNet, Marking\n",
    "from pm4py.objects.log.obj import EventLog, Trace, Event\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_algo\n",
    "import logging\n",
    "\n",
    "# הגדרת לוגר להדפסת זמני ריצה בזמן אמת\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# יבוא פונקציות מ-icpm_experiments\n",
    "sys.path.append(\"src\")\n",
    "from icpm_experiments import load_and_preprocess_log, generate_model_from_file\n",
    "\n",
    "# התקנת חבילות נדרשות\n",
    "packages = {\n",
    "    \"pulp\": \"pulp\",\n",
    "    \"pm4py\": \"pm4py\",\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"requests\": \"requests\"\n",
    "}\n",
    "for module_name, pip_name in packages.items():\n",
    "    if importlib.util.find_spec(module_name) is None:\n",
    "        print(f\"Installing {pip_name}...\")\n",
    "        os.system(f\"pip install {pip_name}\")\n",
    "    else:\n",
    "        print(f\"{pip_name} already installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f8471f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_synchronous_product(model_net, model_marking, trace):\n",
    "    sp_net = PetriNet(\"Synchronous Product\")\n",
    "    place_map = {}\n",
    "\n",
    "    for p in model_net.places:\n",
    "        new_p = PetriNet.Place(f\"{p.name}_m\")\n",
    "        sp_net.places.add(new_p)\n",
    "        place_map[p.name] = new_p\n",
    "\n",
    "    log_places = []\n",
    "    for i in range(len(trace) + 1):\n",
    "        p_log = PetriNet.Place(f\"p{i}_l\")\n",
    "        sp_net.places.add(p_log)\n",
    "        log_places.append(p_log)\n",
    "\n",
    "    sp_marking = Marking()\n",
    "    for p in model_marking:\n",
    "        sp_marking[place_map[p.name]] = 1\n",
    "    sp_marking[log_places[0]] = 1\n",
    "\n",
    "    for t in model_net.transitions:\n",
    "        label = t.label if t.label is not None else \"τ\"\n",
    "        for i, event in enumerate(trace):\n",
    "            if t.label == event:\n",
    "                sync_t = PetriNet.Transition(f\"{label}_S_{i}\", f\"{label},{label}\")\n",
    "                sp_net.transitions.add(sync_t)\n",
    "                for arc in t.in_arcs:\n",
    "                    petri_utils.add_arc_from_to(place_map[arc.source.name], sync_t, sp_net)\n",
    "                for arc in t.out_arcs:\n",
    "                    petri_utils.add_arc_from_to(sync_t, place_map[arc.target.name], sp_net)\n",
    "                petri_utils.add_arc_from_to(log_places[i], sync_t, sp_net)\n",
    "                petri_utils.add_arc_from_to(sync_t, log_places[i + 1], sp_net)\n",
    "\n",
    "    for t in model_net.transitions:\n",
    "        label = t.label if t.label is not None else \"τ\"\n",
    "        if label == \"τ\":\n",
    "            move_m = PetriNet.Transition(f\"{label}\", f\"{label},{label}\")\n",
    "        else:\n",
    "            move_m = PetriNet.Transition(f\"{label}_M\", f\"{label},>>\")\n",
    "        sp_net.transitions.add(move_m)\n",
    "        for arc in t.in_arcs:\n",
    "            petri_utils.add_arc_from_to(place_map[arc.source.name], move_m, sp_net)\n",
    "        for arc in t.out_arcs:\n",
    "            petri_utils.add_arc_from_to(move_m, place_map[arc.target.name], sp_net)\n",
    "\n",
    "    for i, event in enumerate(trace):\n",
    "        label = event if event is not None else \"τ\"\n",
    "        move_l = PetriNet.Transition(f\"{label}_L_{i}\", f\">>,{label}\")\n",
    "        sp_net.transitions.add(move_l)\n",
    "        petri_utils.add_arc_from_to(log_places[i], move_l, sp_net)\n",
    "        petri_utils.add_arc_from_to(move_l, log_places[i + 1], sp_net)\n",
    "\n",
    "    return sp_net, sp_marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5b0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sorted_incidence_matrix(net):\n",
    "    places = list(net.places)\n",
    "    transitions = list(net.transitions)\n",
    "    move_model, move_sync, move_log = [], [], []\n",
    "    \n",
    "    for t in transitions:\n",
    "        if \">>\" in t.label and t.label.endswith(\">>\"):\n",
    "            move_model.append(t)\n",
    "        elif \">>\" in t.label and t.label.startswith(\">>\"):\n",
    "            move_log.append(t)\n",
    "        elif \",\" in t.label and \">>\" not in t.label:\n",
    "            move_sync.append(t)\n",
    "    \n",
    "    sorted_transitions = move_model + move_sync + move_log\n",
    "    place_names = [p.name for p in places]\n",
    "    place_index = {name: i for i, name in enumerate(place_names)}\n",
    "    matrix = np.zeros((len(places), len(sorted_transitions)), dtype=int)\n",
    "    \n",
    "    for j, t in enumerate(sorted_transitions):\n",
    "        for arc in t.in_arcs:\n",
    "            i = place_index[arc.source.name]\n",
    "            matrix[i, j] -= 1\n",
    "        for arc in t.out_arcs:\n",
    "            i = place_index[arc.target.name]\n",
    "            matrix[i, j] += 1\n",
    "    \n",
    "    return matrix, place_names, [t.name for t in sorted_transitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c530f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sync_initial_marking_vector(sync_net, model_marking, trace):\n",
    "    place_names = [p.name for p in sync_net.places]\n",
    "    vector = np.zeros(len(place_names), dtype=int)\n",
    "    for p, tokens in model_marking.items():\n",
    "        place_name = f\"{p.name}_m\"\n",
    "        if place_name in place_names:\n",
    "            idx = place_names.index(place_name)\n",
    "            vector[idx] = tokens\n",
    "    log_place = \"p0_l\"\n",
    "    if log_place in place_names:\n",
    "        vector[place_names.index(log_place)] = 1\n",
    "    return vector\n",
    "\n",
    "def get_sync_final_marking(sync_net, model_final_marking, trace):\n",
    "    place_names = [p.name for p in sync_net.places]\n",
    "    vector = np.zeros(len(place_names), dtype=int)\n",
    "    for p, tokens in model_final_marking.items():\n",
    "        place_name = f\"{p.name}_m\"\n",
    "        if place_name in place_names:\n",
    "            idx = place_names.index(place_name)\n",
    "            vector[idx] = tokens\n",
    "    log_place = f\"p{len(trace)}_l\"\n",
    "    if log_place in place_names:\n",
    "        vector[place_names.index(log_place)] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1433dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_incremental_lp(C, m0_vector, x, cost_map, mf, row_names, col_names, integer=True):\n",
    "    prob = pulp.LpProblem(\"Incremental_Alignment\", pulp.LpMinimize)\n",
    "    num_places, num_trans = len(row_names), len(col_names)\n",
    "    cat = \"Binary\" if integer else \"Continuous\"\n",
    "    \n",
    "    # הגדרת משתנים עם חסם תחתון של 0\n",
    "    y = {(i, t): pulp.LpVariable(f\"y_{i}_{t}\", lowBound=0, cat=cat) for i in range(x) for t in range(num_trans)}\n",
    "    m = {(i, p): pulp.LpVariable(f\"m_{i}_{row_names[p]}\", lowBound=0, cat=\"Integer\") for i in range(x + 1) for p in range(num_places)}\n",
    "    \n",
    "    # הדפסת עלויות לבדיקה\n",
    "    logging.info(f\"Cost map: {{k: v for k, v in cost_map.items() if v != 0}}\")\n",
    "    \n",
    "    # אילוץ סימון התחלתי\n",
    "    for p in range(num_places):\n",
    "        prob += m[(0, p)] == m0_vector[p], f\"initial_marking_p{p}\"\n",
    "    \n",
    "    # פונקציית מטרה\n",
    "    prob += pulp.lpSum([cost_map[col_names[t]] * y[(i, t)] for i in range(x) for t in range(num_trans)])\n",
    "    \n",
    "    # אילוצים\n",
    "    for i in range(x):\n",
    "        # אילוץ: לכל היותר מעבר אחד בכל שלב\n",
    "        prob += pulp.lpSum([y[(i, t)] for t in range(num_trans)]) <= 1, f\"one_move_step_{i}\"\n",
    "        # אילוץ: בדיקת אפשרות של מעבר\n",
    "        for t in range(num_trans):\n",
    "            for p in range(num_places):\n",
    "                if C[p, t] < 0:\n",
    "                    prob += m[(i, p)] >= abs(C[p, t]) * y[(i, t)], f\"enabled_check_p{p}_t{t}_step{i}\"\n",
    "        # עדכון סימונים\n",
    "        for p in range(num_places):\n",
    "            delta = pulp.lpSum([C[p, t] * y[(i, t)] for t in range(num_trans)])\n",
    "            prob += m[(i + 1, p)] == m[(i, p)] + delta, f\"marking_update_p{p}_step_{i}\"\n",
    "    \n",
    "    # אילוץ סימון סופי\n",
    "    for p in range(len(mf)):\n",
    "        prob += m[(x, p)] == mf[p], f\"final_marking_p{p}\"\n",
    "    \n",
    "    # אילוץ נוסף: מגביל את סך המעברים\n",
    "    prob += pulp.lpSum([y[(i, t)] for i in range(x) for t in range(num_trans)]) <= x, f\"total_transitions_bound\"\n",
    "    \n",
    "    return prob, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d6f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pulp\n",
    "import time\n",
    "from io import StringIO\n",
    "from pm4py.objects.petri_net.utils import petri_utils\n",
    "from pm4py.objects.petri_net.obj import PetriNet, Marking\n",
    "from pm4py.objects.log.obj import EventLog, Trace, Event\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_algo\n",
    "import threading\n",
    "\n",
    "# Set console encoding to utf-8 to avoid UnicodeEncodeError\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "\n",
    "# Configure logging without timestamp\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger()\n",
    "# Remove existing handlers to avoid duplicate output\n",
    "logger.handlers = []\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "logger.addHandler(console_handler)\n",
    "# File handler\n",
    "file_handler = logging.FileHandler('log_output.txt', mode='a', encoding='utf-8')\n",
    "file_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Import functions from icpm_experiments\n",
    "sys.path.append(\"src\")\n",
    "from icpm_experiments import load_and_preprocess_log, generate_model_from_file\n",
    "\n",
    "# Function to determine the rerun suffix\n",
    "def get_rerun_suffix(model_dir, base_filename):\n",
    "    existing_files = [f for f in os.listdir(model_dir) if f.startswith(base_filename) and f.endswith('.csv')]\n",
    "    rerun_nums = [int(re.search(r'\\((\\d+)\\)\\.csv$', f).group(1)) for f in existing_files if re.search(r'\\((\\d+)\\)\\.csv$', f)]\n",
    "    return f'({max(rerun_nums) + 1})' if rerun_nums else '(2)'\n",
    "\n",
    "# Function to select model and datasets\n",
    "def select_model_and_datasets(data_dir=\"./pr\"):\n",
    "    models = [\n",
    "        \"pr-1-11-1244-A59.txt\",\n",
    "        \"pr-3-11-1151-A48.txt\",\n",
    "        \"pr-3-11-1908-A32.txt\",\n",
    "        \"pr-8-11-1912-A57.txt\"\n",
    "    ]\n",
    "    datasets = {\n",
    "        \"pr-1-11-1244-A59.txt\": [\n",
    "            \"pr-1-11-1244-A59_m17_l1.csv\",\n",
    "            \"pr-1-11-1244-A59_m17_l1_noise.csv\",\n",
    "            \"pr-1-11-1244-A59_m29_l2.csv\",\n",
    "            \"pr-1-11-1244-A59_m29_l2_noise.csv\",\n",
    "            \"pr-1-11-1244-A59_m41_l3.csv\",\n",
    "            \"pr-1-11-1244-A59_m41_l3_noise.csv\",\n",
    "            \"pr-1-11-1244-A59_m55_l4.csv\",\n",
    "            \"pr-1-11-1244-A59_m55_l4_noise.csv\"\n",
    "        ],\n",
    "        \"pr-3-11-1151-A48.txt\": [\n",
    "            \"pr-3-11-1151-A48_m12_l1.csv\",\n",
    "            \"pr-3-11-1151-A48_m12_l1_noise.csv\",\n",
    "            \"pr-3-11-1151-A48_m23_l2.csv\",\n",
    "            \"pr-3-11-1151-A48_m23_l2_noise.csv\",\n",
    "            \"pr-3-11-1151-A48_m37_l3.csv\",\n",
    "            \"pr-3-11-1151-A48_m37_l3_noise.csv\",\n",
    "            \"pr-3-11-1151-A48_m50_l4.csv\",\n",
    "            \"pr-3-11-1151-A48_m50_l4_noise.csv\"\n",
    "        ],\n",
    "        \"pr-3-11-1908-A32.txt\": [\n",
    "            \"pr-3-11-1908-A32_m18_l1.csv\",\n",
    "            \"pr-3-11-1908-A32_m18_l1_noise.csv\",\n",
    "            \"pr-3-11-1908-A32_m27_l2.csv\",\n",
    "            \"pr-3-11-1908-A32_m27_l2_noise.csv\",\n",
    "            \"pr-3-11-1908-A32_m34_l3.csv\",\n",
    "            \"pr-3-11-1908-A32_m34_l3_noise.csv\",\n",
    "            \"pr-3-11-1908-A32_m41_l4.csv\",\n",
    "            \"pr-3-11-1908-A32_m41_l4_noise.csv\"\n",
    "        ],\n",
    "        \"pr-8-11-1912-A57.txt\": [\n",
    "            \"pr-8-11-1912-A57_m15_l1.csv\",\n",
    "            \"pr-8-11-1912-A57_m15_l1_noise.csv\",\n",
    "            \"pr-8-11-1912-A57_m26_l2.csv\",\n",
    "            \"pr-8-11-1912-A57_m26_l2_noise.csv\",\n",
    "            \"pr-8-11-1912-A57_m39_l3.csv\",\n",
    "            \"pr-8-11-1912-A57_m39_l3_noise.csv\",\n",
    "            \"pr-8-11-1912-A57_m52_l4.csv\",\n",
    "            \"pr-8-11-1912-A57_m52_l4_noise.csv\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n=== Step 1: Select a Model ===\")\n",
    "    print(\"Available process models:\")\n",
    "    for i, model in enumerate(models, 1):\n",
    "        print(f\"{i}. {model}\")\n",
    "    print(\"Please select a model by entering its number (e.g., '2' for pr-3-11-1151-A48.txt).\")\n",
    "    while True:\n",
    "        sys.stdout.flush()\n",
    "        choice = input(\"Model number (1-4): \").strip()\n",
    "        try:\n",
    "            choice = int(choice)\n",
    "            if 1 <= choice <= len(models):\n",
    "                selected_model = models[choice-1]\n",
    "                break\n",
    "            print(f\"Invalid choice. Please enter a number between 1 and {len(models)}.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "    \n",
    "    model_dir = selected_model.replace('.txt', '')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    results_file = os.path.join(model_dir, f\"results_final_{model_dir}.csv\")\n",
    "    \n",
    "    # Check if model file exists\n",
    "    model_path = os.path.join(data_dir, selected_model)\n",
    "    if not os.path.exists(model_path):\n",
    "        logging.error(f\"Model file {selected_model} not found in {data_dir}. Please ensure it exists.\")\n",
    "        print(f\"Error: Model file {selected_model} not found in {data_dir}. Please ensure it exists.\")\n",
    "        sys.stdout.flush()\n",
    "        return None, [], None\n",
    "    \n",
    "    completed_datasets = []\n",
    "    incomplete_datasets = []\n",
    "    dataset_status = {}\n",
    "    valid_datasets = [ds for ds in datasets[selected_model] if os.path.exists(os.path.join(data_dir, ds))]\n",
    "    \n",
    "    for dataset in valid_datasets:\n",
    "        try:\n",
    "            # Read dataset to get total number of traces\n",
    "            df = pd.read_csv(os.path.join(data_dir, dataset), encoding='utf-8-sig')\n",
    "            total_traces = len(df.groupby('case:concept:name'))\n",
    "            \n",
    "            # Check temp_results file for last processed trace\n",
    "            temp_results_file = os.path.join(model_dir, f\"temp_results_{dataset.replace('.csv', '')}.csv\")\n",
    "            last_trace = 0\n",
    "            if os.path.exists(temp_results_file):\n",
    "                try:\n",
    "                    temp_results_df = pd.read_csv(temp_results_file, encoding='utf-8-sig')\n",
    "                    if 'Trace Range' in temp_results_df.columns:\n",
    "                        valid_ranges = []\n",
    "                        for trace_range in temp_results_df['Trace Range']:\n",
    "                            if isinstance(trace_range, str) and '-' in trace_range:\n",
    "                                try:\n",
    "                                    end_trace = int(trace_range.split('-')[-1])\n",
    "                                    valid_ranges.append(end_trace)\n",
    "                                except (ValueError, TypeError) as e:\n",
    "                                    logging.warning(f\"Invalid Trace Range value in {temp_results_file}: '{trace_range}'. Skipping.\")\n",
    "                                    continue\n",
    "                        last_trace = max(valid_ranges) if valid_ranges else 0\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error reading {temp_results_file}: {e}. Trying with 'latin1' encoding.\")\n",
    "                    try:\n",
    "                        temp_results_df = pd.read_csv(temp_results_file, encoding='latin1')\n",
    "                        if 'Trace Range' in temp_results_df.columns:\n",
    "                            valid_ranges = []\n",
    "                            for trace_range in temp_results_df['Trace Range']:\n",
    "                                if isinstance(trace_range, str) and '-' in trace_range:\n",
    "                                    try:\n",
    "                                        end_trace = int(trace_range.split('-')[-1])\n",
    "                                        valid_ranges.append(end_trace)\n",
    "                                    except (ValueError, TypeError) as e:\n",
    "                                        logging.warning(f\"Invalid Trace Range value in {temp_results_file}: '{trace_range}'. Skipping.\")\n",
    "                                        continue\n",
    "                            last_trace = max(valid_ranges) if valid_ranges else 0\n",
    "                    except Exception as e2:\n",
    "                        logging.warning(f\"Failed with 'latin1' encoding: {e2}. Assuming no progress.\")\n",
    "            \n",
    "            # Determine status\n",
    "            if last_trace >= total_traces and last_trace > 0:\n",
    "                completed_datasets.append(dataset)\n",
    "                dataset_status[dataset] = \"(Completed)\"\n",
    "            else:\n",
    "                incomplete_datasets.append(dataset)\n",
    "                dataset_status[dataset] = f\"(In progress, last trace: {last_trace})\" if last_trace > 0 else \"\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error processing dataset {dataset}: {e}. Assuming incomplete.\")\n",
    "            incomplete_datasets.append(dataset)\n",
    "            dataset_status[dataset] = \"\"\n",
    "    \n",
    "    print(f\"\\n=== Step 2: Review Datasets for Model {selected_model} ===\")\n",
    "    print(\"Available datasets:\")\n",
    "    for i, dataset in enumerate(valid_datasets, 1):\n",
    "        print(f\"{i}. {dataset} {dataset_status.get(dataset, '')}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if not incomplete_datasets and not completed_datasets:\n",
    "        print(\"No datasets available for this model in the local directory. Exiting.\")\n",
    "        sys.stdout.flush()\n",
    "        return None, [], None\n",
    "    \n",
    "    print(\"\\n=== Step 3: Choose Processing Option ===\")\n",
    "    if incomplete_datasets:\n",
    "        print(f\"Continue from the last trace of the first incomplete dataset ({incomplete_datasets[0]})? (y/n)\")\n",
    "        print(f\"  - This will append results to existing files (e.g., temp_results_{incomplete_datasets[0].replace('.csv', '')}.csv).\")\n",
    "    else:\n",
    "        print(\"No incomplete datasets. Would you like to select a specific dataset to process? (y/n)\")\n",
    "    while True:\n",
    "        sys.stdout.flush()\n",
    "        choice = input(\"Enter y or n: \").strip().lower()\n",
    "        if choice in ['y', 'n']:\n",
    "            break\n",
    "        print(\"Please enter 'y' or 'n'.\")\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    if choice == 'y' and incomplete_datasets:\n",
    "        return selected_model, incomplete_datasets, model_dir\n",
    "    else:\n",
    "        print(\"\\n=== Step 4: Select a Dataset ===\")\n",
    "        print(\"Please select a dataset by entering its number.\")\n",
    "        for i, dataset in enumerate(valid_datasets, 1):\n",
    "            print(f\"{i}. {dataset} {dataset_status.get(dataset, '')}\")\n",
    "        sys.stdout.flush()\n",
    "        if not valid_datasets:\n",
    "            print(\"No valid datasets found in the local directory. Exiting.\")\n",
    "            sys.stdout.flush()\n",
    "            return None, [], None\n",
    "        while True:\n",
    "            sys.stdout.flush()\n",
    "            dataset_choice = input(f\"Dataset number (1-{len(valid_datasets)}): \").strip()\n",
    "            try:\n",
    "                dataset_choice = int(dataset_choice)\n",
    "                if 1 <= dataset_choice <= len(valid_datasets):\n",
    "                    selected_dataset = valid_datasets[dataset_choice-1]\n",
    "                    break\n",
    "                print(f\"Invalid choice. Please enter a number between 1 and {len(valid_datasets)}.\")\n",
    "                sys.stdout.flush()\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number.\")\n",
    "                sys.stdout.flush()\n",
    "        \n",
    "        temp_results_file = os.path.join(model_dir, f\"temp_results_{selected_dataset.replace('.csv', '')}.csv\")\n",
    "        integer_lp_file = os.path.join(model_dir, f\"integer_lp_traces_{selected_dataset.replace('.csv', '')}.csv\")\n",
    "        results_file = os.path.join(model_dir, f\"results_final_{model_dir}.csv\")\n",
    "        start_from_scratch = False\n",
    "        start_trace_idx = 1\n",
    "        \n",
    "        # Get the number of traces in the selected dataset\n",
    "        try:\n",
    "            logging.info(f\"Loading dataset {selected_dataset} to determine total traces...\")\n",
    "            df = pd.read_csv(os.path.join(data_dir, selected_dataset), encoding='utf-8-sig')\n",
    "            total_traces = len(df.groupby('case:concept:name'))\n",
    "            logging.info(f\"Dataset {selected_dataset} contains {total_traces} traces.\")\n",
    "            sys.stdout.flush()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load dataset {selected_dataset}: {e}. Assuming max 10000 traces.\")\n",
    "            total_traces = 10000\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        if selected_dataset in completed_datasets:\n",
    "            print(f\"\\n=== Warning: Dataset {selected_dataset} Already Completed ===\")\n",
    "            print(f\"This dataset has already processed {total_traces} traces. Results will be saved in new files:\")\n",
    "            print(f\"  - temp_results_{selected_dataset.replace('.csv', '')}(n).csv\")\n",
    "            print(f\"  - integer_lp_traces_{selected_dataset.replace('.csv', '')}(n).csv\")\n",
    "            print(f\"  - results_final_{model_dir}(n).csv\")\n",
    "            sys.stdout.flush()\n",
    "            rerun_suffix = get_rerun_suffix(model_dir, f\"temp_results_{selected_dataset.replace('.csv', '')}\")\n",
    "            temp_results_file = os.path.join(model_dir, f\"temp_results_{selected_dataset.replace('.csv', '')}{rerun_suffix}.csv\")\n",
    "            integer_lp_file = os.path.join(model_dir, f\"integer_lp_traces_{selected_dataset.replace('.csv', '')}{rerun_suffix}.csv\")\n",
    "            results_file = os.path.join(model_dir, f\"results_final_{model_dir}{rerun_suffix}.csv\")\n",
    "            start_from_scratch = True\n",
    "        elif os.path.exists(temp_results_file):\n",
    "            try:\n",
    "                temp_results_df = pd.read_csv(temp_results_file, encoding='utf-8-sig')\n",
    "                valid_ranges = []\n",
    "                for trace_range in temp_results_df['Trace Range']:\n",
    "                    if isinstance(trace_range, str) and '-' in trace_range:\n",
    "                        try:\n",
    "                            end_trace = int(trace_range.split('-')[-1])\n",
    "                            valid_ranges.append(end_trace)\n",
    "                        except (ValueError, TypeError) as e:\n",
    "                            logging.warning(f\"Invalid Trace Range value in {temp_results_file}: '{trace_range}'. Skipping.\")\n",
    "                            continue\n",
    "                max_trace = max(valid_ranges) if valid_ranges else 0\n",
    "                print(f\"\\nDataset {selected_dataset} is in progress. Last processed trace: {max_trace}.\")\n",
    "                print(f\"Dataset contains {total_traces} traces.\")\n",
    "                print(f\"Please enter the starting trace number (1 to {total_traces}, or 0 to continue from {max_trace + 1}):\")\n",
    "                sys.stdout.flush()\n",
    "                while True:\n",
    "                    sys.stdout.flush()\n",
    "                    trace_choice = input(f\"Starting trace number: \").strip()\n",
    "                    try:\n",
    "                        trace_choice = int(trace_choice)\n",
    "                        if trace_choice == 0:\n",
    "                            start_trace_idx = max_trace + 1\n",
    "                            break\n",
    "                        if 1 <= trace_choice <= total_traces:\n",
    "                            start_trace_idx = trace_choice\n",
    "                            break\n",
    "                        print(f\"Invalid choice. Please enter a number between 1 and {total_traces}, or 0 to continue from {max_trace + 1}.\")\n",
    "                        sys.stdout.flush()\n",
    "                    except ValueError:\n",
    "                        print(\"Please enter a valid number.\")\n",
    "                        sys.stdout.flush()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error reading {temp_results_file}: {e}. Trying with 'latin1' encoding.\")\n",
    "                try:\n",
    "                    temp_results_df = pd.read_csv(temp_results_file, encoding='latin1')\n",
    "                    valid_ranges = []\n",
    "                    for trace_range in temp_results_df['Trace Range']:\n",
    "                        if isinstance(trace_range, str) and '-' in trace_range:\n",
    "                            try:\n",
    "                                end_trace = int(trace_range.split('-')[-1])\n",
    "                                valid_ranges.append(end_trace)\n",
    "                            except (ValueError, TypeError) as e:\n",
    "                                logging.warning(f\"Invalid Trace Range value in {temp_results_file}: '{trace_range}'. Skipping.\")\n",
    "                                continue\n",
    "                    max_trace = max(valid_ranges) if valid_ranges else 0\n",
    "                    print(f\"\\nDataset {selected_dataset} is in progress. Last processed trace: {max_trace}.\")\n",
    "                    print(f\"Dataset contains {total_traces} traces.\")\n",
    "                    print(f\"Please enter the starting trace number (1 to {total_traces}, or 0 to continue from {max_trace + 1}):\")\n",
    "                    sys.stdout.flush()\n",
    "                    while True:\n",
    "                        sys.stdout.flush()\n",
    "                        trace_choice = input(f\"Starting trace number: \").strip()\n",
    "                        try:\n",
    "                            trace_choice = int(trace_choice)\n",
    "                            if trace_choice == 0:\n",
    "                                start_trace_idx = max_trace + 1\n",
    "                                break\n",
    "                            if 1 <= trace_choice <= total_traces:\n",
    "                                start_trace_idx = trace_choice\n",
    "                                break\n",
    "                            print(f\"Invalid choice. Please enter a number between 1 and {total_traces}, or 0 to continue from {max_trace + 1}.\")\n",
    "                            sys.stdout.flush()\n",
    "                        except ValueError:\n",
    "                            print(\"Please enter a valid number.\")\n",
    "                            sys.stdout.flush()\n",
    "                except Exception as e2:\n",
    "                    logging.warning(f\"Failed with 'latin1' encoding: {e2}. Starting from trace 1.\")\n",
    "                    print(f\"\\nDataset {selected_dataset} contains {total_traces} traces.\")\n",
    "                    print(f\"Please enter the starting trace number (1 to {total_traces}):\")\n",
    "                    sys.stdout.flush()\n",
    "                    while True:\n",
    "                        sys.stdout.flush()\n",
    "                        trace_choice = input(f\"Starting trace number: \").strip()\n",
    "                        try:\n",
    "                            trace_choice = int(trace_choice)\n",
    "                            if 1 <= trace_choice <= total_traces:\n",
    "                                start_trace_idx = trace_choice\n",
    "                                break\n",
    "                            print(f\"Invalid choice. Please enter a number between 1 and {total_traces}.\")\n",
    "                            sys.stdout.flush()\n",
    "                        except ValueError:\n",
    "                            print(\"Please enter a valid number.\")\n",
    "                            sys.stdout.flush()\n",
    "        else:\n",
    "            print(f\"\\nDataset {selected_dataset} contains {total_traces} traces.\")\n",
    "            print(f\"Please enter the starting trace number (1 to {total_traces}):\")\n",
    "            sys.stdout.flush()\n",
    "            while True:\n",
    "                sys.stdout.flush()\n",
    "                trace_choice = input(f\"Starting trace number: \").strip()\n",
    "                try:\n",
    "                    trace_choice = int(trace_choice)\n",
    "                    if 1 <= trace_choice <= total_traces:\n",
    "                        start_trace_idx = trace_choice\n",
    "                        break\n",
    "                    print(f\"Invalid choice. Please enter a number between 1 and {total_traces}.\")\n",
    "                    sys.stdout.flush()\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a valid number.\")\n",
    "                    sys.stdout.flush()\n",
    "        \n",
    "        return selected_model, [selected_dataset], model_dir, temp_results_file, integer_lp_file, results_file, start_from_scratch, start_trace_idx\n",
    "\n",
    "# Function to process a single dataset\n",
    "def process_single_dataset(model_file, trace_file, model_dir, temp_results_file=None, integer_lp_file=None, results_file=None, start_from_scratch=False, start_trace_idx=1, data_dir=\"./pr\"):\n",
    "    # Initialize counters\n",
    "    astar_times, astar_costs, lp_times, lp_costs, lp_success, continuous_lp_success = [], [], [], [], 0, 0\n",
    "    integer_lp_traces = []\n",
    "    \n",
    "    if temp_results_file is None:\n",
    "        temp_results_file = os.path.join(model_dir, f\"temp_results_{trace_file.replace('.csv', '')}.csv\")\n",
    "    if integer_lp_file is None:\n",
    "        integer_lp_file = os.path.join(model_dir, f\"integer_lp_traces_{trace_file.replace('.csv', '')}.csv\")\n",
    "    if results_file is None:\n",
    "        results_file = os.path.join(model_dir, f\"results_final_{model_dir}.csv\")\n",
    "    \n",
    "    logging.info(f\"\\n=== Processing model: {model_file}, trace file: {trace_file} ===\")\n",
    "    logging.info(f\"Results will be saved to: {temp_results_file}, {results_file}, {integer_lp_file}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Extract noise level\n",
    "    noise_level_match = re.search(r'_l(\\d+)', trace_file)\n",
    "    noise_level = int(noise_level_match.group(1)) if noise_level_match else 1\n",
    "    time_limit = {1: 420, 2: 500, 3: 700, 4:700}.get(noise_level, 700)\n",
    "    batch_size = {1: 200, 2: 20, 3: 5, 4: 5}.get(noise_level, 5)\n",
    "    logging.info(f\"Noise level: l{noise_level}, Time limit per trace: {time_limit}s, Batch size: {batch_size}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Load files\n",
    "    model_path = os.path.join(data_dir, model_file)\n",
    "    trace_path = os.path.join(data_dir, trace_file)\n",
    "    \n",
    "    try:\n",
    "        # Check if model file exists\n",
    "        if not os.path.exists(model_path):\n",
    "            logging.error(f\"Model file {model_file} not found in {data_dir}.\")\n",
    "            sys.stdout.flush()\n",
    "            return {'status': 'failed', 'trace_file': trace_file}\n",
    "        logging.info(\"Loading model file...\")\n",
    "        with open(model_path, 'r', encoding='utf-8') as f:\n",
    "            model_content = f.read()\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Check if trace file exists\n",
    "        if not os.path.exists(trace_path):\n",
    "            logging.error(f\"Trace file {trace_file} not found in {data_dir}.\")\n",
    "            sys.stdout.flush()\n",
    "            return {'status': 'failed', 'trace_file': trace_file}\n",
    "        logging.info(\"Loading trace file...\")\n",
    "        with open(trace_path, 'r', encoding='utf-8') as f:\n",
    "            trace_content = f.read()\n",
    "        sys.stdout.flush()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load files: {e}\")\n",
    "        sys.stdout.flush()\n",
    "        return {'status': 'failed', 'trace_file': trace_file}\n",
    "    \n",
    "    # Read trace file\n",
    "    try:\n",
    "        logging.info(\"Reading trace file into DataFrame...\")\n",
    "        df = pd.read_csv(StringIO(trace_content), encoding='utf-8')\n",
    "        sys.stdout.flush()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to read trace file into DataFrame: {e}\")\n",
    "        sys.stdout.flush()\n",
    "        return {'status': 'failed', 'trace_file': trace_file}\n",
    "    \n",
    "    # Load model\n",
    "    logging.info(\"Creating temporary model file...\")\n",
    "    temp_model_path = os.path.join(model_dir, f\"temp_{model_file}\")\n",
    "    try:\n",
    "        with open(temp_model_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(model_content)\n",
    "        logging.info(\"Generating model from file...\")\n",
    "        model, init_marking, final_marking = generate_model_from_file(temp_model_path)\n",
    "        pm4py_net = model.pm4py_net\n",
    "        pm4py_initial_marking = model.pm4py_initial_marking\n",
    "        pm4py_final_marking = model.pm4py_final_marking\n",
    "        sys.stdout.flush()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate model: {e}\")\n",
    "        if os.path.exists(temp_model_path):\n",
    "            os.remove(temp_model_path)\n",
    "        sys.stdout.flush()\n",
    "        return {'status': 'failed', 'trace_file': trace_file}\n",
    "    \n",
    "    # Process traces\n",
    "    logging.info(\"Grouping traces by case:concept:name...\")\n",
    "    try:\n",
    "        traces = df.groupby('case:concept:name')['Activity'].apply(list).to_dict()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to group traces: {e}\")\n",
    "        if os.path.exists(temp_model_path):\n",
    "            os.remove(temp_model_path)\n",
    "        sys.stdout.flush()\n",
    "        return {'status': 'failed', 'trace_file': trace_file}\n",
    "    \n",
    "    num_traces = len(traces)\n",
    "    avg_trace_length = np.mean([len(t) for t in traces.values()]) if traces else 0\n",
    "    \n",
    "    logging.info(f\"Found {num_traces} traces with average length {avg_trace_length:.2f}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Check processed traces\n",
    "    if not start_from_scratch and os.path.exists(temp_results_file):\n",
    "        try:\n",
    "            temp_results_df = pd.read_csv(temp_results_file, encoding='utf-8-sig')\n",
    "            processed_ranges = temp_results_df[temp_results_df['Dataset'] == trace_file]['Trace Range']\n",
    "            if not processed_ranges.empty:\n",
    "                valid_ranges = []\n",
    "                for trace_range in processed_ranges:\n",
    "                    if isinstance(trace_range, str) and '-' in trace_range:\n",
    "                        try:\n",
    "                            end_trace = int(trace_range.split('-')[-1])\n",
    "                            valid_ranges.append(end_trace)\n",
    "                        except (ValueError, TypeError) as e:\n",
    "                            logging.warning(f\"Invalid Trace Range value in {temp_results_file}: '{trace_range}'. Skipping.\")\n",
    "                            continue\n",
    "                if valid_ranges:\n",
    "                    end_trace = max(valid_ranges)\n",
    "                    if end_trace >= num_traces:\n",
    "                        logging.info(f\"All {num_traces} traces already processed for {trace_file}\")\n",
    "                        if os.path.exists(temp_model_path):\n",
    "                            os.remove(temp_model_path)\n",
    "                        sys.stdout.flush()\n",
    "                        return {'status': 'completed', 'trace_file': trace_file}\n",
    "                    if start_trace_idx == 1:\n",
    "                        start_trace_idx = end_trace + 1\n",
    "                        logging.info(f\"Found processed traces up to {end_trace}, starting from trace {start_trace_idx}\")\n",
    "                        sys.stdout.flush()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error reading {temp_results_file}: {e}. Trying with 'latin1' encoding.\")\n",
    "            try:\n",
    "                temp_results_df = pd.read_csv(temp_results_file, encoding='latin1')\n",
    "                processed_ranges = temp_results_df[temp_results_df['Dataset'] == trace_file]['Trace Range']\n",
    "                if not processed_ranges.empty:\n",
    "                    valid_ranges = []\n",
    "                    for trace_range in processed_ranges:\n",
    "                        if isinstance(trace_range, str) and '-' in trace_range:\n",
    "                            try:\n",
    "                                end_trace = int(trace_range.split('-')[-1])\n",
    "                                valid_ranges.append(end_trace)\n",
    "                            except (ValueError, TypeError) as e:\n",
    "                                logging.warning(f\"Invalid Trace Range value in {temp_results_file}: '{trace_range}'. Skipping.\")\n",
    "                                continue\n",
    "                    if valid_ranges:\n",
    "                        end_trace = max(valid_ranges)\n",
    "                        if end_trace >= num_traces:\n",
    "                            logging.info(f\"All {num_traces} traces already processed for {trace_file}\")\n",
    "                            if os.path.exists(temp_model_path):\n",
    "                                os.remove(temp_model_path)\n",
    "                            sys.stdout.flush()\n",
    "                            return {'status': 'completed', 'trace_file': trace_file}\n",
    "                        if start_trace_idx == 1:\n",
    "                            start_trace_idx = end_trace + 1\n",
    "                            logging.info(f\"Found processed traces up to {end_trace}, starting from trace {start_trace_idx}\")\n",
    "                            sys.stdout.flush()\n",
    "            except Exception as e2:\n",
    "                logging.error(f\"Failed to read {temp_results_file} with 'latin1': {e2}\")\n",
    "                sys.stdout.flush()\n",
    "    \n",
    "    # Validate start_trace_idx\n",
    "    if start_trace_idx > num_traces:\n",
    "        logging.error(f\"Starting trace index {start_trace_idx} exceeds total traces {num_traces}. Starting from trace 1.\")\n",
    "        start_trace_idx = 1\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    # Variables for temporary results\n",
    "    temp_astar_times, temp_astar_costs, temp_lp_times, temp_lp_costs, temp_lp_success, temp_continuous_lp_success = [], [], [], [], 0, 0\n",
    "    temp_trace_ids = []\n",
    "    temp_integer_lp_traces = []\n",
    "    batch_start_idx = start_trace_idx\n",
    "    \n",
    "    for trace_idx, (trace_id, trace) in enumerate(traces.items(), 1):\n",
    "        if trace_idx < start_trace_idx:\n",
    "            continue\n",
    "        \n",
    "        logging.info(f\"\\nProcessing trace {trace_idx}/{num_traces} (ID: {trace_id})\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        try:\n",
    "            # A* Alignment\n",
    "            trace_obj = Trace([{\"concept:name\": act} for act in trace])\n",
    "            event_log = EventLog([trace_obj])\n",
    "            start_astar = time.time()\n",
    "            alignment_result = alignments_algo.apply(event_log, pm4py_net, pm4py_initial_marking, pm4py_final_marking)[0]\n",
    "            end_astar = time.time()\n",
    "            astar_cost = alignment_result['cost'] / 10000\n",
    "            astar_time = end_astar - start_astar\n",
    "            temp_astar_times.append(astar_time)\n",
    "            temp_astar_costs.append(astar_cost)\n",
    "            temp_trace_ids.append(trace_id)\n",
    "            logging.info(f\"A* - Time: {astar_time:.2f}s, Cost: {astar_cost:.2f}\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # Build synchronous product\n",
    "            sync_net, sync_marking = build_synchronous_product(pm4py_net, pm4py_initial_marking, trace)\n",
    "            logging.info(f\"Synchronous net: {len(sync_net.places)} places, {len(sync_net.transitions)} transitions\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            C_np, row_names, col_names = compute_sorted_incidence_matrix(sync_net)\n",
    "            logging.info(f\"Incidence matrix shape: {C_np.shape}\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            m0 = get_sync_initial_marking_vector(sync_net, pm4py_initial_marking, trace)\n",
    "            mf = get_sync_final_marking(sync_net, pm4py_final_marking, trace)\n",
    "            cost_map = {t.name: 0 if t.label.split(\",\")[0] == t.label.split(\",\")[1] else 1 for t in sync_net.transitions}\n",
    "            \n",
    "            # Dynamic upper bound\n",
    "            margin = {1: 1 if astar_cost == 0 else 3, 2: 1 if astar_cost == 0 else 3, 3: 3 if astar_cost == 0 else 5, 4: 5 if astar_cost == 0 else 5}.get(noise_level, 2 if astar_cost == 0 else 5)\n",
    "            x = len(trace) + int(astar_cost) + margin\n",
    "            logging.info(f\"Setting upper bound x = {x} (trace length: {len(trace)}, margin: {margin})\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # Continuous LP\n",
    "            logging.info(\"Running Continuous LP...\")\n",
    "            sys.stdout.flush()\n",
    "            prob, y_vars, m_vars = build_incremental_lp(C_np, m0, x, cost_map, mf, row_names, col_names, integer=False)\n",
    "            start_lp = time.time()\n",
    "            lp_thread = threading.Thread(target=lambda: prob.solve(pulp.PULP_CBC_CMD(timeLimit=time_limit, msg=False, options=['-maxSeconds', str(time_limit)])))\n",
    "            lp_thread.start()\n",
    "            notification_triggered = False\n",
    "            run_integer_lp = False\n",
    "            \n",
    "            while lp_thread.is_alive():\n",
    "                elapsed = time.time() - start_lp\n",
    "                if elapsed > time_limit * 0.75 and not notification_triggered:\n",
    "                    logging.info(f\"Continuous LP - Elapsed: {elapsed:.2f}s\")\n",
    "                    notification_triggered = True\n",
    "                    sys.stdout.flush()\n",
    "                time.sleep(0.1)\n",
    "                if elapsed > time_limit + 10:\n",
    "                    logging.warning(f\"Continuous LP for trace {trace_idx}/{num_traces} exceeded time limit by 10s, forcing termination\")\n",
    "                    temp_lp_times.append(elapsed)\n",
    "                    run_integer_lp = True\n",
    "                    lp_thread.join(1)\n",
    "                    sys.stdout.flush()\n",
    "                    break\n",
    "            \n",
    "            end_lp = time.time()\n",
    "            lp_time = end_lp - start_lp\n",
    "            lp_status = pulp.LpStatus[prob.status]\n",
    "            lp_cost = None\n",
    "            \n",
    "            if lp_status == \"Optimal\":\n",
    "                lp_cost = pulp.value(prob.objective)\n",
    "                logging.info(f\"Continuous LP - Time: {lp_time:.2f}s, Cost: {lp_cost:.2f}, Status: {lp_status}\")\n",
    "                if abs(lp_cost - astar_cost) < 1e-6:\n",
    "                    temp_lp_times.append(lp_time)\n",
    "                    temp_lp_costs.append(lp_cost)\n",
    "                    temp_lp_success += 1\n",
    "                    temp_continuous_lp_success += 1\n",
    "                else:\n",
    "                    logging.info(\"Continuous LP cost differs from A*, running Integer LP...\")\n",
    "                    run_integer_lp = True\n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                logging.info(f\"Continuous LP - Time: {lp_time:.2f}s, Status: {lp_status} (Failed to converge)\")\n",
    "                temp_lp_times.append(lp_time)\n",
    "                run_integer_lp = True\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            # Integer LP\n",
    "            if run_integer_lp:\n",
    "                logging.info(\"Running Integer LP...\")\n",
    "                sys.stdout.flush()\n",
    "                prob, y_vars, m_vars = build_incremental_lp(C_np, m0, x, cost_map, mf, row_names, col_names, integer=True)\n",
    "                start_lp = time.time()\n",
    "                lp_thread = threading.Thread(target=lambda: prob.solve(pulp.PULP_CBC_CMD(timeLimit=time_limit, msg=False, options=['-maxSeconds', str(time_limit)])))\n",
    "                lp_thread.start()\n",
    "                notification_triggered = False\n",
    "                \n",
    "                while lp_thread.is_alive():\n",
    "                    elapsed = time.time() - start_lp\n",
    "                    if elapsed > time_limit * 0.75 and not notification_triggered:\n",
    "                        logging.info(f\"Integer LP - Elapsed: {elapsed:.2f}s\")\n",
    "                        notification_triggered = True\n",
    "                        sys.stdout.flush()\n",
    "                    time.sleep(0.1)\n",
    "                    if elapsed > time_limit + 10:\n",
    "                        logging.warning(f\"Integer LP for trace {trace_idx}/{num_traces} exceeded time limit by 10s, forcing termination\")\n",
    "                        temp_lp_times.append(elapsed)\n",
    "                        temp_integer_lp_traces.append(trace_id)\n",
    "                        lp_thread.join(1)\n",
    "                        sys.stdout.flush()\n",
    "                        break\n",
    "                else:\n",
    "                    end_lp = time.time()\n",
    "                    lp_time = end_lp - start_lp\n",
    "                    lp_status = pulp.LpStatus[prob.status]\n",
    "                    if lp_status == \"Optimal\":\n",
    "                        lp_cost = pulp.value(prob.objective)\n",
    "                        temp_lp_times.append(lp_time)\n",
    "                        temp_lp_costs.append(lp_cost)\n",
    "                        temp_lp_success += 1\n",
    "                        temp_integer_lp_traces.append(trace_id)\n",
    "                        logging.info(f\"Integer LP - Time: {lp_time:.2f}s, Cost: {lp_cost:.2f}, Status: {lp_status}\")\n",
    "                    else:\n",
    "                        temp_lp_times.append(lp_time)\n",
    "                        temp_integer_lp_traces.append(trace_id)\n",
    "                        logging.info(f\"Integer LP - Time: {lp_time:.2f}s, Status: {lp_status} (Failed to converge)\")\n",
    "                    sys.stdout.flush()\n",
    "            \n",
    "            # Update global counters only if trace processing completes\n",
    "            astar_times.extend(temp_astar_times)\n",
    "            astar_costs.extend(temp_astar_costs)\n",
    "            lp_times.extend(temp_lp_times)\n",
    "            lp_costs.extend(temp_lp_costs)\n",
    "            lp_success += temp_lp_success\n",
    "            continuous_lp_success += temp_continuous_lp_success\n",
    "            integer_lp_traces.extend(temp_integer_lp_traces)\n",
    "            \n",
    "            # Save temporary results\n",
    "            if len(temp_astar_times) >= batch_size or trace_idx == num_traces:\n",
    "                save_results(trace_file, trace_idx, num_traces, avg_trace_length, astar_times, astar_costs, lp_times, lp_costs, lp_success, continuous_lp_success, temp_astar_times, temp_astar_costs, temp_lp_times, temp_lp_costs, temp_lp_success, temp_continuous_lp_success, results_file, temp_results_file, batch_start_idx)\n",
    "                save_integer_lp_traces(temp_integer_lp_traces, trace_file, integer_lp_file, batch_start_idx, trace_idx)\n",
    "                temp_astar_times, temp_astar_costs, temp_lp_times, temp_lp_costs, temp_lp_success, temp_continuous_lp_success = [], [], [], [], 0, 0\n",
    "                temp_integer_lp_traces = []\n",
    "                temp_trace_ids = []\n",
    "                batch_start_idx = trace_idx + 1\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            logging.info(f\"Interrupted at trace {trace_idx}/{num_traces}\")\n",
    "            temp_lp_times.append(time.time() - start_lp if 'start_lp' in locals() else 0)\n",
    "            if temp_astar_times:\n",
    "                save_results(trace_file, trace_idx, num_traces, avg_trace_length, astar_times, astar_costs, lp_times, lp_costs, lp_success, continuous_lp_success, temp_astar_times, temp_astar_costs, temp_lp_times, temp_lp_costs, temp_lp_success, temp_continuous_lp_success, results_file, temp_results_file, batch_start_idx)\n",
    "                save_integer_lp_traces(temp_integer_lp_traces, trace_file, integer_lp_file, batch_start_idx, trace_idx)\n",
    "            if os.path.exists(temp_model_path):\n",
    "                os.remove(temp_model_path)\n",
    "            sys.stdout.flush()\n",
    "            return {'status': 'interrupted', 'trace_file': trace_file}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing trace {trace_idx}/{num_traces}: {e}\")\n",
    "            temp_astar_times, temp_astar_costs, temp_lp_times, temp_lp_costs, temp_lp_success, temp_continuous_lp_success = [], [], [], [], 0, 0\n",
    "            temp_integer_lp_traces = []\n",
    "            temp_trace_ids = []\n",
    "            sys.stdout.flush()\n",
    "            continue\n",
    "    \n",
    "    # Save final results\n",
    "    if astar_times:  # Save only if at least one trace was processed\n",
    "        save_results(trace_file, trace_idx, num_traces, avg_trace_length, astar_times, astar_costs, lp_times, lp_costs, lp_success, continuous_lp_success, temp_astar_times, temp_astar_costs, temp_lp_times, temp_lp_costs, temp_lp_success, temp_continuous_lp_success, results_file, temp_results_file, batch_start_idx)\n",
    "        save_integer_lp_traces(integer_lp_traces, trace_file, integer_lp_file, 1, num_traces)\n",
    "    \n",
    "    if os.path.exists(temp_model_path):\n",
    "        os.remove(temp_model_path)\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "    return {'status': 'completed', 'trace_file': trace_file}\n",
    "\n",
    "# Function to save temporary results\n",
    "def save_results(trace_file, trace_idx, num_traces, avg_trace_length, astar_times, astar_costs, lp_times, lp_costs, lp_success, continuous_lp_success, temp_astar_times, temp_astar_costs, temp_lp_times, temp_lp_costs, temp_lp_success, temp_continuous_lp_success, results_file, temp_results_file, batch_start_idx):\n",
    "    temp_avg_astar_time = np.mean(temp_astar_times) if temp_astar_times else 0\n",
    "    temp_avg_astar_cost = np.mean(temp_astar_costs) if temp_astar_costs else 0\n",
    "    temp_avg_lp_time = np.mean([t for t in temp_lp_times if t is not None]) if any(t is not None for t in temp_lp_times) else 0\n",
    "    temp_avg_lp_cost = np.mean([c for c in temp_lp_costs if c is not None]) if any(c is not None for c in temp_lp_costs) else None\n",
    "    temp_lp_success_rate = (temp_lp_success / len(temp_astar_times)) * 100 if len(temp_astar_times) > 0 else 0\n",
    "    temp_continuous_lp_success_rate = (temp_continuous_lp_success / len(temp_astar_times)) * 100 if len(temp_astar_times) > 0 else 0\n",
    "    \n",
    "    temp_result = {\n",
    "        \"Dataset\": trace_file,\n",
    "        \"Trace Range\": f\"{batch_start_idx}-{trace_idx}\",\n",
    "        \"Number of Traces\": len(temp_astar_times),\n",
    "        \"Average Trace Length\": avg_trace_length,\n",
    "        \"Average A* Time (s)\": temp_avg_astar_time,\n",
    "        \"Average A* Cost\": temp_avg_astar_cost,\n",
    "        \"LP Success Rate (%)\": temp_lp_success_rate,\n",
    "        \"Continuous LP Success Rate (%)\": temp_continuous_lp_success_rate,\n",
    "        \"Average LP Time (s)\": temp_avg_lp_time,\n",
    "        \"Average LP Cost\": temp_avg_lp_cost\n",
    "    }\n",
    "    \n",
    "    if os.path.exists(temp_results_file):\n",
    "        try:\n",
    "            temp_results_df = pd.read_csv(temp_results_file, encoding='utf-8-sig')\n",
    "            temp_results_df = pd.concat([temp_results_df, pd.DataFrame([temp_result])], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to update {temp_results_file}: {e}. Trying with 'latin1' encoding.\")\n",
    "            try:\n",
    "                temp_results_df = pd.read_csv(temp_results_file, encoding='latin1')\n",
    "                temp_results_df = pd.concat([temp_results_df, pd.DataFrame([temp_result])], ignore_index=True)\n",
    "            except Exception as e2:\n",
    "                logging.error(f\"Failed with 'latin1' encoding: {e2}. Creating new file.\")\n",
    "                temp_results_df = pd.DataFrame([temp_result])\n",
    "    else:\n",
    "        temp_results_df = pd.DataFrame([temp_result])\n",
    "    temp_results_df.to_csv(temp_results_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    logging.info(f\"\\n✅ === Temporary results for traces {batch_start_idx}-{trace_idx} saved to {temp_results_file} === ✅\")\n",
    "    logging.info(f\"Temporary result: {temp_result}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Function to save integer LP traces\n",
    "def save_integer_lp_traces(integer_lp_traces, trace_file, integer_lp_file, batch_start_idx, trace_idx):\n",
    "    if not integer_lp_traces:\n",
    "        return\n",
    "    integer_result = {\n",
    "        \"Dataset\": trace_file,\n",
    "        \"Trace Range\": f\"{batch_start_idx}-{trace_idx}\",\n",
    "        \"Integer LP Trace IDs\": \",\".join(integer_lp_traces)\n",
    "    }\n",
    "    if os.path.exists(integer_lp_file):\n",
    "        try:\n",
    "            integer_df = pd.read_csv(integer_lp_file, encoding='utf-8-sig')\n",
    "            integer_df = pd.concat([integer_df, pd.DataFrame([integer_result])], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to update {integer_lp_file}: {e}. Trying with 'latin1' encoding.\")\n",
    "            try:\n",
    "                integer_df = pd.read_csv(integer_lp_file, encoding='latin1')\n",
    "                integer_df = pd.concat([integer_df, pd.DataFrame([integer_result])], ignore_index=True)\n",
    "            except Exception as e2:\n",
    "                logging.error(f\"Failed with 'latin1' encoding: {e2}. Creating new file.\")\n",
    "                integer_df = pd.DataFrame([integer_result])\n",
    "    else:\n",
    "        integer_df = pd.DataFrame([integer_result])\n",
    "    integer_df.to_csv(integer_lp_file, index=False, encoding='utf-8-sig')\n",
    "    logging.info(f\"Saved integer LP traces to {integer_lp_file}: {integer_result}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Function to save final results\n",
    "def save_final_results(trace_file, num_traces, avg_trace_length, astar_times, astar_costs, lp_times, lp_costs, lp_success, continuous_lp_success, results_file):\n",
    "    avg_astar_time = np.mean(astar_times) if astar_times else 0\n",
    "    avg_astar_cost = np.mean(astar_costs) if astar_costs else 0\n",
    "    avg_lp_time = np.mean([t for t in lp_times if t is not None]) if any(t is not None for t in lp_times) else 0\n",
    "    avg_lp_cost = np.mean([c for c in lp_costs if c is not None]) if any(c is not None for c in lp_costs) else None\n",
    "    lp_success_rate = (lp_success / num_traces) * 100 if num_traces > 0 else 0\n",
    "    continuous_lp_success_rate = (continuous_lp_success / num_traces) * 100 if num_traces > 0 else 0\n",
    "    \n",
    "    logging.info(f\"\\nSummary for {trace_file}:\")\n",
    "    logging.info(f\"Number of Traces: {num_traces}\")\n",
    "    logging.info(f\"Average Trace Length: {avg_trace_length:.2f}\")\n",
    "    logging.info(f\"Average A* Time: {avg_astar_time:.2f}s\")\n",
    "    logging.info(f\"Average A* Cost: {avg_astar_cost:.2f}\")\n",
    "    logging.info(f\"LP Success Rate: {lp_success_rate:.2f}%\")\n",
    "    logging.info(f\"Continuous LP Success Rate: {continuous_lp_success_rate:.2f}%\")\n",
    "    logging.info(f\"Average LP Time: {avg_lp_time:.2f}s\")\n",
    "    logging.info(f\"Average LP Cost: {'N/A' if avg_lp_cost is None else f'{avg_lp_cost:.2f}'}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    result = {\n",
    "        \"Dataset\": trace_file,\n",
    "        \"Number of Traces\": num_traces,\n",
    "        \"Average Trace Length\": avg_trace_length,\n",
    "        \"Average A* Time (s)\": avg_astar_time,\n",
    "        \"Average A* Cost\": avg_astar_cost,\n",
    "        \"LP Success Rate (%)\": lp_success_rate,\n",
    "        \"Continuous LP Success Rate (%)\": continuous_lp_success_rate,\n",
    "        \"Average LP Time (s)\": avg_lp_time,\n",
    "        \"Average LP Cost\": avg_lp_cost\n",
    "    }\n",
    "    \n",
    "    if os.path.exists(results_file):\n",
    "        try:\n",
    "            results_df = pd.read_csv(results_file, encoding='utf-8-sig')\n",
    "            results_df = results_df[results_df['Dataset'] != trace_file]\n",
    "            results_df = pd.concat([results_df, pd.DataFrame([result])], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to update {results_file}: {e}. Trying with 'latin1' encoding.\")\n",
    "            try:\n",
    "                results_df = pd.read_csv(results_file, encoding='latin1')\n",
    "                results_df = results_df[results_df['Dataset'] != trace_file]\n",
    "                results_df = pd.concat([results_df, pd.DataFrame([result])], ignore_index=True)\n",
    "            except Exception as e2:\n",
    "                logging.error(f\"Failed with 'latin1' encoding: {e2}. Creating new file.\")\n",
    "                results_df = pd.DataFrame([result])\n",
    "    else:\n",
    "        results_df = pd.DataFrame([result])\n",
    "    results_df.to_csv(results_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    logging.info(f\"Final result for dataset: {result}\")\n",
    "    logging.info(f\"\\n✅ === Dataset {trace_file} Completed! Results Saved to {results_file} === ✅\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6147ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1: Select a Model ===\n",
      "Available process models:\n",
      "1. pr-1-11-1244-A59.txt\n",
      "2. pr-3-11-1151-A48.txt\n",
      "3. pr-3-11-1908-A32.txt\n",
      "4. pr-8-11-1912-A57.txt\n",
      "Please select a model by entering its number (e.g., '2' for pr-3-11-1151-A48.txt).\n",
      "\n",
      "=== Step 2: Review Datasets for Model pr-1-11-1244-A59.txt ===\n",
      "Available datasets:\n",
      "1. pr-1-11-1244-A59_m17_l1.csv \n",
      "2. pr-1-11-1244-A59_m17_l1_noise.csv \n",
      "3. pr-1-11-1244-A59_m29_l2.csv (Completed)\n",
      "4. pr-1-11-1244-A59_m29_l2_noise.csv (In progress, last trace: 996)\n",
      "5. pr-1-11-1244-A59_m41_l3.csv \n",
      "6. pr-1-11-1244-A59_m41_l3_noise.csv \n",
      "7. pr-1-11-1244-A59_m55_l4.csv (In progress, last trace: 183)\n",
      "8. pr-1-11-1244-A59_m55_l4_noise.csv \n",
      "\n",
      "=== Step 3: Choose Processing Option ===\n",
      "Continue from the last trace of the first incomplete dataset (pr-1-11-1244-A59_m17_l1.csv)? (y/n)\n",
      "  - This will append results to existing files (e.g., temp_results_pr-1-11-1244-A59_m17_l1.csv).\n",
      "\n",
      "=== Step 4: Select a Dataset ===\n",
      "Please select a dataset by entering its number.\n",
      "1. pr-1-11-1244-A59_m17_l1.csv \n",
      "2. pr-1-11-1244-A59_m17_l1_noise.csv \n",
      "3. pr-1-11-1244-A59_m29_l2.csv (Completed)\n",
      "4. pr-1-11-1244-A59_m29_l2_noise.csv (In progress, last trace: 996)\n",
      "5. pr-1-11-1244-A59_m41_l3.csv \n",
      "6. pr-1-11-1244-A59_m41_l3_noise.csv \n",
      "7. pr-1-11-1244-A59_m55_l4.csv (In progress, last trace: 183)\n",
      "8. pr-1-11-1244-A59_m55_l4_noise.csv \n",
      "Loading dataset pr-1-11-1244-A59_m29_l2_noise.csv to determine total traces...\n",
      "Dataset pr-1-11-1244-A59_m29_l2_noise.csv contains 2000 traces.\n",
      "\n",
      "Dataset pr-1-11-1244-A59_m29_l2_noise.csv is in progress. Last processed trace: 996.\n",
      "Dataset contains 2000 traces.\n",
      "Please enter the starting trace number (1 to 2000, or 0 to continue from 997):\n",
      "\n",
      "=== Processing model: pr-1-11-1244-A59.txt, trace file: pr-1-11-1244-A59_m29_l2_noise.csv ===\n",
      "Results will be saved to: pr-1-11-1244-A59\\temp_results_pr-1-11-1244-A59_m29_l2_noise.csv, pr-1-11-1244-A59\\results_final_pr-1-11-1244-A59.csv, pr-1-11-1244-A59\\integer_lp_traces_pr-1-11-1244-A59_m29_l2_noise.csv\n",
      "Noise level: l2, Time limit per trace: 300s, Batch size: 20\n",
      "Loading model file...\n",
      "Loading trace file...\n",
      "Reading trace file into DataFrame...\n",
      "Creating temporary model file...\n",
      "Generating model from file...\n",
      "Grouping traces by case:concept:name...\n",
      "Found 2000 traces with average length 29.23\n",
      "\n",
      "Processing trace 996/2000 (ID: trace_1894)\n",
      "A* - Time: 0.06s, Cost: 0.00\n",
      "Synchronous net: 93 places, 110 transitions\n",
      "Incidence matrix shape: (93, 110)\n",
      "Setting upper bound x = 22 (trace length: 21, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 11.18s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 997/2000 (ID: trace_1895)\n",
      "A* - Time: 1.07s, Cost: 4.00\n",
      "Synchronous net: 115 places, 154 transitions\n",
      "Incidence matrix shape: (115, 154)\n",
      "Setting upper bound x = 50 (trace length: 43, margin: 3)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Elapsed: 225.05s\n",
      "Continuous LP for trace 997/2000 exceeded time limit by 10s, forcing termination\n",
      "Continuous LP - Time: 311.17s, Status: Not Solved (Failed to converge)\n",
      "Running Integer LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Integer LP - Elapsed: 225.09s\n",
      "Integer LP for trace 997/2000 exceeded time limit by 10s, forcing termination\n",
      "\n",
      "Processing trace 998/2000 (ID: trace_1896)\n",
      "A* - Time: 0.06s, Cost: 0.00\n",
      "Synchronous net: 99 places, 122 transitions\n",
      "Incidence matrix shape: (99, 122)\n",
      "Setting upper bound x = 28 (trace length: 27, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 29.75s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 999/2000 (ID: trace_1897)\n",
      "A* - Time: 0.05s, Cost: 0.00\n",
      "Synchronous net: 93 places, 110 transitions\n",
      "Incidence matrix shape: (93, 110)\n",
      "Setting upper bound x = 22 (trace length: 21, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 13.45s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1000/2000 (ID: trace_1898)\n",
      "A* - Time: 0.06s, Cost: 0.00\n",
      "Synchronous net: 84 places, 92 transitions\n",
      "Incidence matrix shape: (84, 92)\n",
      "Setting upper bound x = 13 (trace length: 12, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 4.43s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1001/2000 (ID: trace_1899)\n",
      "A* - Time: 0.05s, Cost: 0.00\n",
      "Synchronous net: 90 places, 104 transitions\n",
      "Incidence matrix shape: (90, 104)\n",
      "Setting upper bound x = 19 (trace length: 18, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 6.65s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1002/2000 (ID: trace_19)\n",
      "A* - Time: 0.05s, Cost: 0.00\n",
      "Synchronous net: 93 places, 110 transitions\n",
      "Incidence matrix shape: (93, 110)\n",
      "Setting upper bound x = 22 (trace length: 21, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 16.62s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1003/2000 (ID: trace_190)\n",
      "A* - Time: 0.03s, Cost: 0.00\n",
      "Synchronous net: 84 places, 92 transitions\n",
      "Incidence matrix shape: (84, 92)\n",
      "Setting upper bound x = 13 (trace length: 12, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 6.04s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1004/2000 (ID: trace_1900)\n",
      "A* - Time: 0.03s, Cost: 0.00\n",
      "Synchronous net: 90 places, 104 transitions\n",
      "Incidence matrix shape: (90, 104)\n",
      "Setting upper bound x = 19 (trace length: 18, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 6.52s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1005/2000 (ID: trace_1901)\n",
      "A* - Time: 0.53s, Cost: 4.00\n",
      "Synchronous net: 119 places, 162 transitions\n",
      "Incidence matrix shape: (119, 162)\n",
      "Setting upper bound x = 54 (trace length: 47, margin: 3)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Elapsed: 225.11s\n",
      "Continuous LP for trace 1005/2000 exceeded time limit by 10s, forcing termination\n",
      "Continuous LP - Time: 311.20s, Status: Not Solved (Failed to converge)\n",
      "Running Integer LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Integer LP - Elapsed: 225.02s\n",
      "Integer LP for trace 1005/2000 exceeded time limit by 10s, forcing termination\n",
      "\n",
      "Processing trace 1006/2000 (ID: trace_1902)\n",
      "A* - Time: 0.67s, Cost: 4.00\n",
      "Synchronous net: 120 places, 164 transitions\n",
      "Incidence matrix shape: (120, 164)\n",
      "Setting upper bound x = 55 (trace length: 48, margin: 3)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Elapsed: 785.42s\n",
      "Continuous LP for trace 1006/2000 exceeded time limit by 10s, forcing termination\n",
      "Continuous LP - Time: 786.60s, Status: Not Solved (Failed to converge)\n",
      "Running Integer LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Integer LP - Elapsed: 225.07s\n",
      "Integer LP for trace 1006/2000 exceeded time limit by 10s, forcing termination\n",
      "\n",
      "Processing trace 1007/2000 (ID: trace_1903)\n",
      "A* - Time: 0.07s, Cost: 0.00\n",
      "Synchronous net: 85 places, 94 transitions\n",
      "Incidence matrix shape: (85, 94)\n",
      "Setting upper bound x = 14 (trace length: 13, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 7.62s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1008/2000 (ID: trace_1904)\n",
      "A* - Time: 0.06s, Cost: 0.00\n",
      "Synchronous net: 89 places, 102 transitions\n",
      "Incidence matrix shape: (89, 102)\n",
      "Setting upper bound x = 18 (trace length: 17, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 4.77s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1009/2000 (ID: trace_1905)\n",
      "A* - Time: 0.08s, Cost: 0.00\n",
      "Synchronous net: 94 places, 112 transitions\n",
      "Incidence matrix shape: (94, 112)\n",
      "Setting upper bound x = 23 (trace length: 22, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 9.59s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1010/2000 (ID: trace_1906)\n",
      "A* - Time: 1.99s, Cost: 4.00\n",
      "Synchronous net: 115 places, 154 transitions\n",
      "Incidence matrix shape: (115, 154)\n",
      "Setting upper bound x = 50 (trace length: 43, margin: 3)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Elapsed: 225.07s\n",
      "Continuous LP for trace 1010/2000 exceeded time limit by 10s, forcing termination\n",
      "Continuous LP - Time: 311.21s, Status: Not Solved (Failed to converge)\n",
      "Running Integer LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Integer LP - Elapsed: 225.08s\n",
      "Integer LP for trace 1010/2000 exceeded time limit by 10s, forcing termination\n",
      "\n",
      "Processing trace 1011/2000 (ID: trace_1907)\n",
      "A* - Time: 2.22s, Cost: 4.00\n",
      "Synchronous net: 121 places, 166 transitions\n",
      "Incidence matrix shape: (121, 166)\n",
      "Setting upper bound x = 56 (trace length: 49, margin: 3)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Elapsed: 225.07s\n",
      "Continuous LP for trace 1011/2000 exceeded time limit by 10s, forcing termination\n",
      "Continuous LP - Time: 311.15s, Status: Not Solved (Failed to converge)\n",
      "Running Integer LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Integer LP - Elapsed: 225.02s\n",
      "Integer LP for trace 1011/2000 exceeded time limit by 10s, forcing termination\n",
      "\n",
      "Processing trace 1012/2000 (ID: trace_1908)\n",
      "A* - Time: 0.05s, Cost: 0.00\n",
      "Synchronous net: 95 places, 114 transitions\n",
      "Incidence matrix shape: (95, 114)\n",
      "Setting upper bound x = 24 (trace length: 23, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 23.05s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1013/2000 (ID: trace_1909)\n",
      "A* - Time: 0.05s, Cost: 0.00\n",
      "Synchronous net: 90 places, 104 transitions\n",
      "Incidence matrix shape: (90, 104)\n",
      "Setting upper bound x = 19 (trace length: 18, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 12.62s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1014/2000 (ID: trace_191)\n",
      "A* - Time: 0.03s, Cost: 0.00\n",
      "Synchronous net: 84 places, 92 transitions\n",
      "Incidence matrix shape: (84, 92)\n",
      "Setting upper bound x = 13 (trace length: 12, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 9.18s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1015/2000 (ID: trace_1910)\n",
      "A* - Time: 0.04s, Cost: 0.00\n",
      "Synchronous net: 90 places, 104 transitions\n",
      "Incidence matrix shape: (90, 104)\n",
      "Setting upper bound x = 19 (trace length: 18, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 15.67s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "✅ === Temporary results for traces 996-1015 saved to pr-1-11-1244-A59\\temp_results_pr-1-11-1244-A59_m29_l2_noise.csv === ✅\n",
      "Temporary result: {'Dataset': 'pr-1-11-1244-A59_m29_l2_noise.csv', 'Trace Range': '996-1015', 'Number of Traces': 20, 'Average Trace Length': np.float64(29.2305), 'Average A* Time (s)': np.float64(0.36280723810195925), 'Average A* Cost': np.float64(1.0), 'LP Success Rate (%)': 75.0, 'Continuous LP Success Rate (%)': 75.0, 'Average LP Time (s)': np.float64(192.81081736882527), 'Average LP Cost': np.float64(0.0)}\n",
      "Saved integer LP traces to pr-1-11-1244-A59\\integer_lp_traces_pr-1-11-1244-A59_m29_l2_noise.csv: {'Dataset': 'pr-1-11-1244-A59_m29_l2_noise.csv', 'Trace Range': '996-1015', 'Integer LP Trace IDs': 'trace_1895,trace_1901,trace_1902,trace_1906,trace_1907'}\n",
      "\n",
      "Processing trace 1016/2000 (ID: trace_1911)\n",
      "A* - Time: 1.26s, Cost: 4.00\n",
      "Synchronous net: 118 places, 160 transitions\n",
      "Incidence matrix shape: (118, 160)\n",
      "Setting upper bound x = 53 (trace length: 46, margin: 3)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Elapsed: 225.07s\n",
      "Continuous LP for trace 1016/2000 exceeded time limit by 10s, forcing termination\n",
      "Continuous LP - Time: 311.19s, Status: Not Solved (Failed to converge)\n",
      "Running Integer LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Integer LP - Elapsed: 225.01s\n",
      "Integer LP for trace 1016/2000 exceeded time limit by 10s, forcing termination\n",
      "\n",
      "Processing trace 1017/2000 (ID: trace_1912)\n",
      "A* - Time: 0.04s, Cost: 0.00\n",
      "Synchronous net: 84 places, 92 transitions\n",
      "Incidence matrix shape: (84, 92)\n",
      "Setting upper bound x = 13 (trace length: 12, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 4.89s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1018/2000 (ID: trace_1913)\n",
      "A* - Time: 0.04s, Cost: 0.00\n",
      "Synchronous net: 90 places, 104 transitions\n",
      "Incidence matrix shape: (90, 104)\n",
      "Setting upper bound x = 19 (trace length: 18, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 15.26s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1019/2000 (ID: trace_1914)\n",
      "A* - Time: 0.05s, Cost: 0.00\n",
      "Synchronous net: 90 places, 104 transitions\n",
      "Incidence matrix shape: (90, 104)\n",
      "Setting upper bound x = 19 (trace length: 18, margin: 1)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Time: 14.96s, Cost: 0.00, Status: Optimal\n",
      "\n",
      "Processing trace 1020/2000 (ID: trace_1915)\n",
      "A* - Time: 1.36s, Cost: 4.00\n",
      "Synchronous net: 118 places, 160 transitions\n",
      "Incidence matrix shape: (118, 160)\n",
      "Setting upper bound x = 53 (trace length: 46, margin: 3)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Elapsed: 225.02s\n",
      "Continuous LP for trace 1020/2000 exceeded time limit by 10s, forcing termination\n",
      "Continuous LP - Time: 311.15s, Status: Not Solved (Failed to converge)\n",
      "Running Integer LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Integer LP - Elapsed: 225.01s\n",
      "Integer LP for trace 1020/2000 exceeded time limit by 10s, forcing termination\n",
      "\n",
      "Processing trace 1021/2000 (ID: trace_1916)\n",
      "A* - Time: 1.40s, Cost: 4.00\n",
      "Synchronous net: 115 places, 154 transitions\n",
      "Incidence matrix shape: (115, 154)\n",
      "Setting upper bound x = 50 (trace length: 43, margin: 3)\n",
      "Running Continuous LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Continuous LP - Elapsed: 225.11s\n",
      "Continuous LP for trace 1021/2000 exceeded time limit by 10s, forcing termination\n",
      "Continuous LP - Time: 311.19s, Status: Not Solved (Failed to converge)\n",
      "Running Integer LP...\n",
      "Cost map: {k: v for k, v in cost_map.items() if v != 0}\n",
      "Integer LP - Elapsed: 225.10s\n",
      "Interrupted at trace 1021/2000\n",
      "\n",
      "✅ === Temporary results for traces 1016-1021 saved to pr-1-11-1244-A59\\temp_results_pr-1-11-1244-A59_m29_l2_noise.csv === ✅\n",
      "Temporary result: {'Dataset': 'pr-1-11-1244-A59_m29_l2_noise.csv', 'Trace Range': '1016-1021', 'Number of Traces': 6, 'Average Trace Length': np.float64(29.2305), 'Average A* Time (s)': np.float64(0.6907432874043783), 'Average A* Cost': np.float64(2.0), 'LP Success Rate (%)': 50.0, 'Continuous LP Success Rate (%)': 50.0, 'Average LP Time (s)': np.float64(235.29588653643927), 'Average LP Cost': np.float64(0.0)}\n",
      "Saved integer LP traces to pr-1-11-1244-A59\\integer_lp_traces_pr-1-11-1244-A59_m29_l2_noise.csv: {'Dataset': 'pr-1-11-1244-A59_m29_l2_noise.csv', 'Trace Range': '1016-1021', 'Integer LP Trace IDs': 'trace_1911,trace_1915'}\n",
      "✅ Result for pr-1-11-1244-A59_m29_l2_noise.csv: interrupted\n",
      "✅ Program interrupted by user. Restart to continue from last trace.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger()\n",
    "if not any(isinstance(handler, logging.FileHandler) for handler in logger.handlers):\n",
    "    file_handler = logging.FileHandler('log_output.txt', mode='a', encoding='utf-8')\n",
    "    file_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        result = select_model_and_datasets()\n",
    "        if result is None or (len(result) == 3 and not result[1]):\n",
    "            logging.info(\"✅ No datasets available to process. Exiting.\")\n",
    "            break\n",
    "        if len(result) == 3:\n",
    "            model_file, trace_files, model_dir = result\n",
    "            start_from_scratch = False\n",
    "            start_trace_idx = 1\n",
    "        else:\n",
    "            model_file, trace_files, model_dir, temp_results_file, integer_lp_file, results_file, start_from_scratch, start_trace_idx = result\n",
    "        \n",
    "        if not trace_files:\n",
    "            logging.info(f\"✅ No valid datasets available for model {model_file}. Skipping to next model selection.\")\n",
    "            continue\n",
    "        \n",
    "        for trace_file in trace_files:\n",
    "            # Check if dataset is completed\n",
    "            results_file_temp = os.path.join(model_dir, f\"results_final_{model_dir}.csv\")\n",
    "            if os.path.exists(results_file_temp):\n",
    "                try:\n",
    "                    results_df = pd.read_csv(results_file_temp, encoding='utf-8-sig')\n",
    "                    dataset_results = results_df[results_df['Dataset'] == trace_file]\n",
    "                    if not dataset_results.empty:\n",
    "                        try:\n",
    "                            df = pd.read_csv(os.path.join(\"./pr\", trace_file), encoding='utf-8-sig')\n",
    "                            total_traces = len(df.groupby('case:concept:name'))\n",
    "                            if dataset_results['Number of Traces'].iloc[-1] >= total_traces:\n",
    "                                logging.info(f\"✅ Dataset {trace_file} already completed with {total_traces} traces. Skipping.\")\n",
    "                                continue\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Error reading dataset {trace_file} to determine total traces: {e}. Processing dataset.\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error reading {results_file_temp}: {e}. Trying with 'latin1' encoding.\")\n",
    "                    try:\n",
    "                        results_df = pd.read_csv(results_file_temp, encoding='latin1')\n",
    "                        dataset_results = results_df[results_df['Dataset'] == trace_file]\n",
    "                        if not dataset_results.empty:\n",
    "                            try:\n",
    "                                df = pd.read_csv(os.path.join(\"./pr\", trace_file), encoding='utf-8-sig')\n",
    "                                total_traces = len(df.groupby('case:concept:name'))\n",
    "                                if dataset_results['Number of Traces'].iloc[-1] >= total_traces:\n",
    "                                    logging.info(f\"✅ Dataset {trace_file} already completed with {total_traces} traces. Skipping.\")\n",
    "                                    continue\n",
    "                            except Exception as e2:\n",
    "                                logging.warning(f\"Error reading dataset {trace_file} to determine total traces: {e2}. Processing dataset.\")\n",
    "                    except Exception as e2:\n",
    "                        logging.error(f\"Failed with 'latin1' encoding: {e2}. Processing dataset.\")\n",
    "            \n",
    "            if len(result) == 3:\n",
    "                temp_results_file = os.path.join(model_dir, f\"temp_results_{trace_file.replace('.csv', '')}.csv\")\n",
    "                integer_lp_file = os.path.join(model_dir, f\"integer_lp_traces_{trace_file.replace('.csv', '')}.csv\")\n",
    "                results_file = os.path.join(model_dir, f\"results_final_{model_dir}.csv\")\n",
    "            \n",
    "            result = process_single_dataset(\n",
    "                model_file, trace_file, model_dir, \n",
    "                temp_results_file=temp_results_file, \n",
    "                integer_lp_file=integer_lp_file, \n",
    "                results_file=results_file, \n",
    "                start_from_scratch=start_from_scratch, \n",
    "                start_trace_idx=start_trace_idx\n",
    "            )\n",
    "            logging.info(f\"✅ Result for {trace_file}: {result['status']}\")\n",
    "            if result['status'] == 'interrupted':\n",
    "                logging.info(\"✅ Program interrupted by user. Restart to continue from last trace.\")\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d598733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a1d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
